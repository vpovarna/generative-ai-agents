# MCP Integration Test Cases

Test scenarios for eval-agent MCP server integration with Claude Code, Claude Desktop, and Cursor.

## Setup

### Build MCP Server

```bash
cd eval-agent
go build -o bin/eval-mcp cmd/mcp/main.go
```

### Add to Claude Code

```bash
claude mcp add --transport stdio --scope project eval-agent \
  --env AWS_REGION=us-east-1 \
  --env AWS_ACCESS_KEY_ID=your-key \
  --env AWS_SECRET_ACCESS_KEY=your-secret \
  --env CLAUDE_MODEL_ID=us.anthropic.claude-3-5-haiku-20241022-v1:0 \
  -- /path/to/eval-agent/bin/eval-mcp
```

### Verify Installation

```bash
claude mcp list
# Should show: eval-agent (stdio) - Ready
```

---

## Tool Discovery Tests

### Test Case 1: List Available Tools

**Action:** Start Claude Code session and type:
```
/mcp
```

**Expected Output:**
```
MCP Servers:
- eval-agent (stdio) - Ready
  Tools:
  - evaluate_response: Evaluate an AI agent response...
  - evaluate_single_judge: Evaluate with a single judge...
```

**Verification:**
- eval-agent server shows as "Ready"
- Two tools are listed
- Tool descriptions are visible

---

## evaluate_response Tool Tests

### Test Case 2: Basic Evaluation

**Prompt in Claude Code:**
```
Use the evaluate_response tool to evaluate this:

Query: "What is the capital of France?"
Context: "France is a country in Western Europe. Paris is its capital."
Answer: "The capital of France is Paris."
```

**Expected Tool Call:**
```json
{
  "event_id": "<auto-generated>",
  "user_query": "What is the capital of France?",
  "context": "France is a country in Western Europe. Paris is its capital.",
  "answer": "The capital of France is Paris."
}
```

**Expected Response:**
```json
{
  "id": "<event_id>",
  "stages": [
    {"name": "length-checker", "score": 1.0, "...": "..."},
    {"name": "relevance-judge", "score": 0.95, "...": "..."}
  ],
  "confidence": 0.92,
  "verdict": "pass"
}
```

**Verification:**
- Tool executes without errors
- Returns confidence score and verdict
- Claude interprets results and responds naturally

### Test Case 3: Poor Quality Answer

**Prompt:**
```
Evaluate this answer:

Query: "Explain quantum computing"
Context: "Quantum computing uses quantum mechanics for computation"
Answer: "Yes"
```

**Expected Response:**
- Low confidence score (< 0.3)
- `verdict` = "fail"
- Early exit (only prechecks, no LLM judges)

**Verification:**
- Tool detects low-quality answer
- Claude explains why the answer failed

### Test Case 4: Hallucination Detection

**Prompt:**
```
Check if this answer has hallucinations:

Query: "What is the population of Tokyo?"
Context: "Tokyo is the capital of Japan."
Answer: "Tokyo has 50 million people and is the largest city in China."
```

**Expected Response:**
- Low faithfulness score
- Low coherence score
- `verdict` = "fail" or "review"

**Verification:**
- Faithfulness judge detects hallucination
- Claude explains the hallucination issue

### Test Case 5: Missing Context Field

**Prompt:**
```
Evaluate this:

Query: "What is AI?"
Answer: "AI is artificial intelligence."

(Don't provide context)
```

**Expected Behavior:**
- Tool accepts empty context
- Evaluation proceeds normally
- Context field is optional

---

## evaluate_single_judge Tool Tests

### Test Case 6: Relevance Judge Only

**Prompt:**
```
Use evaluate_single_judge to check only relevance:

Query: "What is machine learning?"
Answer: "Machine learning is when computers learn from data."
Judge: relevance
```

**Expected Tool Call:**
```json
{
  "event_id": "<auto-generated>",
  "user_query": "What is machine learning?",
  "answer": "Machine learning is when computers learn from data.",
  "judge_name": "relevance"
}
```

**Expected Response:**
- Only 1 stage (relevance-judge)
- Score > 0.9
- `verdict` = "pass"

**Verification:**
- Only relevance judge runs (fast response)
- No other judges called

### Test Case 7: Custom Threshold

**Prompt:**
```
Check faithfulness with threshold 0.9:

Query: "What is water's boiling point?"
Context: "Water boils at 100Â°C at sea level."
Answer: "Water boils at 100 degrees Celsius."
Judge: faithfulness
Threshold: 0.9
```

**Expected Response:**
- High faithfulness score
- `verdict` = "pass" (score > 0.9)

**Verification:**
- Custom threshold applied correctly
- Pass/fail based on specified threshold

### Test Case 8: Invalid Judge Name

**Prompt:**
```
Evaluate with judge "invalid_judge"
```

**Expected Response:**
- Error: "judge not found: invalid_judge"
- Supported judges listed: relevance, faithfulness, coherence, completeness, instruction

**Verification:**
- Proper error message
- Claude explains valid judge names

---

## Error Handling Tests

### Test Case 9: Missing Required Fields

**Prompt:**
```
Use evaluate_response but don't provide the answer field
```

**Expected Behavior:**
- Tool call fails with validation error
- Claude asks for missing information

### Test Case 10: MCP Server Not Running

**Action:**
1. Stop eval-agent MCP server
2. Try using evaluate_response tool

**Expected Behavior:**
- MCP connection error
- Claude shows "tool unavailable" message
- User can see error in `/mcp` status

---

## Integration Tests

### Test Case 11: Multi-Turn Conversation

**Conversation:**
```
User: Evaluate this answer: Query="What is AI?", Answer="AI is artificial intelligence."

Claude: [Uses tool, gets result]

User: Now check just the relevance score for the same answer

Claude: [Uses evaluate_single_judge with relevance]
```

**Verification:**
- Both tool calls succeed
- Claude maintains context between turns
- Results are consistent

### Test Case 12: Batch Evaluation Request

**Prompt:**
```
Evaluate these 3 answers:
1. Query="What is 2+2?", Answer="4"
2. Query="Capital of Spain?", Answer="Madrid"
3. Query="Who wrote Hamlet?", Answer="Shakespeare"
```

**Expected Behavior:**
- Claude makes 3 separate tool calls
- All evaluations complete
- Claude summarizes results

**Verification:**
- Multiple tool calls work correctly
- Results are aggregated properly

---

## Performance Tests

### Test Case 13: Response Time

**Action:**
```
Time how long this evaluation takes:
Query="What is the speed of light?"
Answer="The speed of light is 299,792,458 meters per second."
```

**Expected:**
- Full pipeline (8 stages) completes in < 5 seconds
- Early exit (3 stages) completes in < 500ms

**Verification:**
- Response time is acceptable
- No timeouts

### Test Case 14: Concurrent Requests

**Prompt:**
```
Evaluate 5 different answers at once:
[provide 5 different query/answer pairs]
```

**Expected Behavior:**
- All 5 evaluations complete
- No conflicts or race conditions
- Results for all 5 provided

---

## Docker MCP Tests

### Test Case 15: Docker MCP Server

**Setup:**
```bash
docker build -t eval-agent-mcp .

# Add to Claude Code with Docker
claude mcp add --transport stdio --scope project eval-agent \
  --env AWS_REGION=us-east-1 \
  --env AWS_ACCESS_KEY_ID=key \
  --env AWS_SECRET_ACCESS_KEY=secret \
  --env CLAUDE_MODEL_ID=model \
  -- docker run -i --rm \
    -e AWS_REGION -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e CLAUDE_MODEL_ID \
    eval-agent-mcp:latest
```

**Test:**
```
Use evaluate_response tool
```

**Expected:**
- Tool works identically to binary version
- Docker container starts/stops correctly
- No environment variable issues

---

## Claude Desktop / Cursor Tests

### Test Case 16: Cursor Integration

**Setup:** Add to Cursor MCP config:
```json
{
  "mcpServers": {
    "eval-agent": {
      "command": "/path/to/eval-agent/bin/eval-mcp"
    }
  }
}
```

**Test:** Use tool in Cursor composer

**Expected:**
- Tool appears in Cursor's tool list
- Works identically to Claude Code

### Test Case 17: Claude Desktop Integration

**Setup:** Add to `claude_desktop_config.json`

**Test:** Use tool in Claude Desktop chat

**Expected:**
- Tool available in Claude Desktop
- Same functionality as CLI

---

## Troubleshooting Test Cases

### Test Case 18: Check MCP Logs

**Action:**
```bash
# Claude Code MCP logs
cat ~/.local/share/claude-code/mcp.log
```

**Verification:**
- Log shows MCP server startup
- No error messages
- Tool calls logged

### Test Case 19: AWS Credentials Issue

**Setup:** Use invalid AWS credentials

**Expected:**
- Tool call fails with AWS auth error
- Error message clearly indicates credential issue

### Test Case 20: Model Not Available

**Setup:** Use unavailable Bedrock model ID

**Expected:**
- Tool call fails
- Error indicates model not found or not enabled

---

## Summary

**Total Test Cases:** 20

**Categories:**
- Tool Discovery: 1 test
- evaluate_response: 4 tests
- evaluate_single_judge: 3 tests
- Error Handling: 2 tests
- Integration: 2 tests
- Performance: 2 tests
- Docker: 1 test
- Platform Integration: 2 tests
- Troubleshooting: 3 tests

**Expected Pass Rate:** 100% with proper setup

## Quick Verification Checklist

- [ ] MCP server shows "Ready" in `/mcp` list
- [ ] Both tools are discoverable
- [ ] Basic evaluation works end-to-end
- [ ] Error messages are clear and helpful
- [ ] Response times are acceptable (< 5s for full pipeline)
- [ ] Works across Claude Code, Desktop, and Cursor
